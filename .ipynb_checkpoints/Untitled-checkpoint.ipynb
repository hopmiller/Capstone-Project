{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6aeed4d6",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f972d1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d36b41",
   "metadata": {},
   "source": [
    "### Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97eb4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('/Users/hopmiller/Desktop/Capstone/DataSets/Tweets.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fc7c4a",
   "metadata": {},
   "source": [
    "### Preview Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e3cd3e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570306133677760513</td>\n",
       "      <td>neutral</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cairdin</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica What @dhepburn said.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:35:52 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570306133677760513           neutral                        1.0000   \n",
       "1  570301130888122368          positive                        0.3486   \n",
       "2  570301083672813571           neutral                        0.6837   \n",
       "3  570301031407624196          negative                        1.0000   \n",
       "4  570300817074462722          negative                        1.0000   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                        NaN  Virgin America   \n",
       "1            NaN                     0.0000  Virgin America   \n",
       "2            NaN                        NaN  Virgin America   \n",
       "3     Bad Flight                     0.7033  Virgin America   \n",
       "4     Can't Tell                     1.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN     cairdin                 NaN              0   \n",
       "1                    NaN    jnardino                 NaN              0   \n",
       "2                    NaN  yvonnalynn                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN    jnardino                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0                @VirginAmerica What @dhepburn said.         NaN   \n",
       "1  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "2  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "3  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "4  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "\n",
       "               tweet_created tweet_location               user_timezone  \n",
       "0  2015-02-24 11:35:52 -0800            NaN  Eastern Time (US & Canada)  \n",
       "1  2015-02-24 11:15:59 -0800            NaN  Pacific Time (US & Canada)  \n",
       "2  2015-02-24 11:15:48 -0800      Lets Play  Central Time (US & Canada)  \n",
       "3  2015-02-24 11:15:36 -0800            NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:14:45 -0800            NaN  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cba9bfdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14640, 15)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shape of data\n",
    "training_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66da3b92",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5584eba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with confidence less than 0.5\n",
    "training_df = training_data.drop(training_data[training_data['airline_sentiment_confidence']<0.5].index, axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97495d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14404, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape after dropping rows\n",
    "training_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc1ef844",
   "metadata": {},
   "outputs": [],
   "source": [
    "## over 200 rows have been dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4473107",
   "metadata": {},
   "source": [
    "### Splitting data into X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee9d6b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = training_df['text']\n",
    "Y = training_df['airline_sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ededa6",
   "metadata": {},
   "source": [
    "### Cleaning X (text data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5709b8ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hopmiller\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hopmiller\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Libraries\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Initializign stop words\n",
    "stop_words = stopwords.words('english')\n",
    "punctuations = string.punctuation\n",
    "\n",
    "#Importing regular expression\n",
    "import re\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7417723",
   "metadata": {},
   "source": [
    "### Standardizing text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daee7cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = []\n",
    "for i in range(len(X)):\n",
    "    text = re.sub('[^a-zA-Z]', ' ',X.iloc[i])\n",
    "# Making all words lowercase\n",
    "    text = text.lower().split()\n",
    "# Lemmatizing words\n",
    "    text = [lemmatizer.lemmatize(word) for word in text if (word not in stop_words) and (word not in punctuations)]\n",
    "    text = ' '.join(text)\n",
    "clean_data.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1cb097e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['americanair ppl need know many seat next flight plz put u standby people next flight']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking cleaned text data (X)\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab2ef7a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         neutral\n",
       "2         neutral\n",
       "3        negative\n",
       "4        negative\n",
       "5        negative\n",
       "           ...   \n",
       "14634    negative\n",
       "14636    negative\n",
       "14637     neutral\n",
       "14638    negative\n",
       "14639     neutral\n",
       "Name: airline_sentiment, Length: 14404, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking sentiment column (Y)\n",
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d35114c",
   "metadata": {},
   "source": [
    "## Defining Sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8672c566",
   "metadata": {},
   "source": [
    "#### Assigning numerical values to the sentiment labels in the \"Y\" column\n",
    "#### negaive = 0\n",
    "##### neutral = 1\n",
    "#### positive = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85a47a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = ['negative' , 'neutral', 'positive']\n",
    "Y = Y.apply(lambda x: sentiments.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc1e4dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "2    1\n",
       "3    0\n",
       "4    0\n",
       "5    0\n",
       "Name: airline_sentiment, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking to make sure sentiments have been defined/labeled correctly\n",
    "Y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "418e4b3a",
   "metadata": {},
   "source": [
    "### Vectorizing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0030c544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 12)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import library \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count_vectorizer = CountVectorizer(max_features = 5000, stop_words = ['virginamerica','united'])\n",
    "X_fit = count_vectorizer.fit_transform(clean_data).toarray()\n",
    "\n",
    "# Check the shape\n",
    "X_fit.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ebc302",
   "metadata": {},
   "source": [
    "### Making a model\n",
    "#### MultinomialNB- type of Naive Bayes Classifier \n",
    "##### Calculates the probability of sentiment based on the probability of words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "631332b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6282d8ae",
   "metadata": {},
   "source": [
    "### Splitting the data  \n",
    "********* errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f0e0e7c9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [1, 14404]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#regressor.fit(x[:, None], y)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Splitting the data into training and testing\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m X_train, X_test, Y_test, Y_train \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_test_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_fit\u001b[49m\u001b[43m,\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtest_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:2417\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_arrays \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   2415\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one array required as input\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2417\u001b[0m arrays \u001b[38;5;241m=\u001b[39m \u001b[43mindexable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2419\u001b[0m n_samples \u001b[38;5;241m=\u001b[39m _num_samples(arrays[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   2420\u001b[0m n_train, n_test \u001b[38;5;241m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2421\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m\n\u001b[0;32m   2422\u001b[0m )\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:378\u001b[0m, in \u001b[0;36mindexable\u001b[1;34m(*iterables)\u001b[0m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;124;03m\"\"\"Make arrays indexable for cross-validation.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03mChecks consistent length, passes through None, and ensures that everything\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;124;03m    sparse matrix, or dataframe) or `None`.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    377\u001b[0m result \u001b[38;5;241m=\u001b[39m [_make_indexable(X) \u001b[38;5;28;01mfor\u001b[39;00m X \u001b[38;5;129;01min\u001b[39;00m iterables]\n\u001b[1;32m--> 378\u001b[0m \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    335\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [1, 14404]"
     ]
    }
   ],
   "source": [
    "#regressor.fit(x[:, None], y)\n",
    "# Splitting the data into training and testing\n",
    "X_train, X_test, Y_test, Y_train = train_test_split(X_fit,Y,test_size=1/3, random_state=0) \n",
    "\n",
    "\n",
    "#X_train, X_test, Y_test, Y_train = train_test_split(X_fit,Y, test_size = 0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7323e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
